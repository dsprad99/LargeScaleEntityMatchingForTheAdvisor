# Large Scale Enttiy for theAdvisor

## About
#### theAdvisor is a project started back in 2013 to help users build a strong biblogrpahy by extneding the document set obtained after a first-level search. While there exists way of looking up papers bibliographys, many of them are innacurate due to sourcing there information utilizing computer vision.

#### Therefore, we have sourced our data from MAG, DBLP, and Citeseer in a diverse set of file formats such as XML, CSV, and SQL. These datasets crossrefernce each other in order to help us gain strong metadata. However, these datasets cannot be matched directly due to there being slight differences in paper titles such as weird spacing, misspellings, capitlization issues, etc. Which is why the two-phase method we used was implemented.

## Phase 1
#### To start off, take one set of data that you would want to match and break it up into a certain number k-mer. The k value in k-mer varies depending on the size of the entitys you want to much but anywhere from 4-8 is recommeneded. Next take the k-mer value and its associated unique key (probably some kind of ID) and add it to the hashmap. Then take the other set of data that needs to be matched and parse through it so that for every single title that must be matched it is broken up into k-mer and found in the hashmap that was just made. Extract all of the unique IDs for each k-mer that the title is broken up into and find the most frequent ID. This will be your match. **Reference the image below that shows how k-mer hashing works**

## Phase 2
#### While Phase 1 is very effecient it does have flaws when matching. Especially when having to match high volume data as often some information loss techniques must be used to give us a feasable runtime to make the project possible. For example when originally evaluating the run times of the k-mer process running at 3-mer alone took about 12.72 seconds per query given a choice of 1,000,000 entites. When removing the top 1000 most freqent 3-mers this reduced the runtime to .69 seconds per query given a choice of 1,000,000 entities. Therefore, given that we were working with such high volumne (given that MAG has 210,000,000 entities) it made sense to deal with some information loss to give us a realistic runtime. However, if we can get the correct entity in the top 10 of the k-mer hashing process we could then use a more expensive and highly accurate algorithm to help make sure that we can confirm we are matching the same entity. The algorithm used in this process is Levenshtein Distance which is a measure of how different two entities are using three different operations. These being swap, delete, and insert. The exact method used in this project was to move the top 10 matches from k-mer hashing onto the levenshtein distance process. During this it would compare the ratio of the candidate entity and the entity that was "matched." If a ratio of .90 or higher was acheived between the two it qualified as a correct match. **Reference the image below that shows how levenhstein distance is produced**
